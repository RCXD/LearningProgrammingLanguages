{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd035c172699b75493cf9b73401c918046e05cb631d4ef3a2eb7e311fe7215ea956",
   "display_name": "Python 3.6.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "35c172699b75493cf9b73401c918046e05cb631d4ef3a2eb7e311fe7215ea956"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Training and Test datasets  \n",
    "이제부터 data와 test로 데이터셋을 반드시 나눔  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "# Evaluation our model using this test dataset\n",
    "x_test = [[2, 1, 1],\n",
    "          [3, 1, 2],\n",
    "          [3, 3, 4]]\n",
    "y_test = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1]]"
   ]
  },
  {
   "source": [
    "Multinomial Classification에 해당하는 모델이므로,   \n",
    "softmax classification과 cross entropy cost함수를 사용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)"
   ]
  },
  {
   "source": [
    "예측값은 원핫인코딩을 시켜 prediction에 저장하고  \n",
    "prediction과 결과값이 같은지 일치하는지 확인(tf.equal() method)  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "source": [
    "학습과정:  \n",
    "Training data만 가지고 학습을 시킨다. Optimizer를 실행할 때 feed해주는 데이터만 확인하면 된다.  \n",
    "이후의 feed는 test데이터를 주고 평가하는 부분   \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Jupyter performance limit 해결:   \n",
    "jupyter.textOutputLimit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "Learning rate: NaN  \n",
    "기존에는 0.1~0.01의 값만 주었음.  \n",
    "1. 너무 크면, 오히려 minimum에서 멀어지는 overshooting(학습(수렴)되지 않고 발산되는)이 발생\n",
    "2. 너무 작으면, 너무 느리게 학습되거나 local minimum에 갇혀버려서 학습이 되지 않을 수 있다.\n",
    "\n",
    "확인하는 방법: "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 2.8577209 [[ 1.2888176  -1.8693092  -1.8167381 ]\n [ 1.6811122   1.8251038  -3.0029712 ]\n [ 0.24722767  1.7532312  -4.524243  ]]\n1 12.50219 [[ 1.2803833 -2.423375  -1.2542381]\n [ 3.1213708 -1.1151547 -1.5029712]\n [ 1.9673705 -1.2794117 -3.2117429]]\n2 26.371323 [[ 0.15538335 -1.8608749  -0.6917381 ]\n [-1.0036292   1.5098453  -0.00297117]\n [-2.1576295   1.5330883  -1.8992429 ]]\n3 13.201395 [[ 0.5299828  -2.7957983  -0.13141418]\n [ 1.433069   -2.4224849   1.4926611 ]\n [ 0.27946925 -2.2143188  -0.58893466]]\n4 14.219437 [[-0.5000918  -2.2332983   0.33616042]\n [-2.4387455   0.20251513  2.7394757 ]\n [-3.6659565   0.59818125  0.5439913 ]]\n5 22.0958 [[-1.2509179e-01 -1.6708227e+00 -6.0131508e-01]\n [-1.2454987e-03  2.8274643e+00 -2.3229737e+00]\n [-1.2284565e+00  3.4106030e+00 -4.7059307e+00]]\n6 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n7 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n8 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n9 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n10 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n11 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n12 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n13 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n14 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n15 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n16 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n17 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n18 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n19 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n20 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n21 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n22 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n23 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n24 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n25 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n26 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n27 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n28 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n29 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n30 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n31 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n32 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n33 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n34 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n35 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n36 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n37 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n38 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n39 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n40 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n41 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n42 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n43 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n44 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n45 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n46 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n47 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n48 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n49 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n50 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n51 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n52 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n53 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n54 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n55 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n56 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n57 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n58 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n59 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n60 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n61 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n62 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n63 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n64 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n65 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n66 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n67 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n68 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n69 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n70 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n71 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n72 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n73 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n74 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n75 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n76 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n77 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n78 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n79 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n80 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n81 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n82 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n83 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n84 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n85 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n86 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n87 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n88 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n89 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n90 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n91 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n92 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n93 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n94 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n95 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n96 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n97 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n98 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n99 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n100 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n101 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n102 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n103 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n104 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n105 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n106 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n107 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n108 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n109 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n110 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n111 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n112 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n113 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n114 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n115 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n116 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n117 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n118 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n119 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n120 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n121 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n122 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n123 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n124 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n125 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n126 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n127 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n128 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n129 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n130 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n131 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n132 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n133 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n134 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n135 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n136 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n137 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n138 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n139 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n140 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n141 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n142 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n143 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n144 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n145 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n146 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n147 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n148 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n149 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n150 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n151 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n152 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n153 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n154 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n155 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n156 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n157 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n158 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n159 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n160 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n161 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n162 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n163 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n164 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n165 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n166 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n167 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n168 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n169 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n170 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n171 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n172 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n173 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n174 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n175 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n176 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n177 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n178 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n179 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n180 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n181 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n182 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n183 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n184 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n185 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n186 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n187 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n188 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n189 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n190 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n191 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n192 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n193 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n194 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n195 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n196 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n197 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n198 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n199 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n200 nan [[nan nan nan]\n [nan nan nan]\n [nan nan nan]]\nPrediction: [0 0 0]\nAccuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "source": [
    "결과값에 nan을 확인할 수 있다.(발산)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "--- \n",
    "learning rate가 너무 작을 경우\n",
    "cost function이 너무 적게 줄어드는 것을 볼 수 있다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "# Try to change learning_rate to small numbers\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-8).minimize(cost)\n",
    "# Correct prediction Test model\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 2.5147169 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104433 -0.2269075 ]\n [ 0.22639586  1.1175408   0.5743505 ]]\n1 2.5147169 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104435 -0.2269075 ]\n [ 0.22639588  1.1175408   0.5743505 ]]\n2 2.5147169 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104436 -0.2269075 ]\n [ 0.22639589  1.1175408   0.5743505 ]]\n3 2.5147169 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104438 -0.2269075 ]\n [ 0.2263959   1.1175408   0.5743505 ]]\n4 2.5147166 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104439 -0.2269075 ]\n [ 0.22639592  1.1175408   0.5743505 ]]\n5 2.5147166 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910444  -0.2269075 ]\n [ 0.22639593  1.1175408   0.5743505 ]]\n6 2.5147166 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104442 -0.2269075 ]\n [ 0.22639595  1.1175408   0.5743505 ]]\n7 2.5147166 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104443 -0.2269075 ]\n [ 0.22639596  1.1175408   0.5743505 ]]\n8 2.5147166 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104445 -0.2269075 ]\n [ 0.22639598  1.1175408   0.5743505 ]]\n9 2.5147164 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104446 -0.2269075 ]\n [ 0.226396    1.1175408   0.5743505 ]]\n10 2.5147164 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104448 -0.2269075 ]\n [ 0.22639601  1.1175408   0.5743505 ]]\n11 2.5147164 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910445  -0.2269075 ]\n [ 0.22639602  1.1175408   0.5743505 ]]\n12 2.5147164 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104451 -0.2269075 ]\n [ 0.22639604  1.1175408   0.5743505 ]]\n13 2.5147164 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104452 -0.2269075 ]\n [ 0.22639605  1.1175408   0.5743505 ]]\n14 2.5147161 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104454 -0.2269075 ]\n [ 0.22639607  1.1175408   0.5743505 ]]\n15 2.5147161 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104455 -0.2269075 ]\n [ 0.22639608  1.1175408   0.5743505 ]]\n16 2.5147161 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104457 -0.2269075 ]\n [ 0.2263961   1.1175408   0.5743505 ]]\n17 2.5147161 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104458 -0.2269075 ]\n [ 0.22639611  1.1175408   0.5743505 ]]\n18 2.5147161 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910446  -0.2269075 ]\n [ 0.22639613  1.1175408   0.5743505 ]]\n19 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104461 -0.2269075 ]\n [ 0.22639614  1.1175408   0.5743505 ]]\n20 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104463 -0.2269075 ]\n [ 0.22639616  1.1175408   0.5743505 ]]\n21 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104464 -0.2269075 ]\n [ 0.22639617  1.1175408   0.5743505 ]]\n22 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104466 -0.2269075 ]\n [ 0.22639619  1.1175408   0.5743505 ]]\n23 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104467 -0.2269075 ]\n [ 0.2263962   1.1175408   0.5743505 ]]\n24 2.514716 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104469 -0.2269075 ]\n [ 0.22639622  1.1175408   0.5743505 ]]\n25 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910447  -0.2269075 ]\n [ 0.22639623  1.1175408   0.5743505 ]]\n26 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104472 -0.2269075 ]\n [ 0.22639625  1.1175408   0.5743505 ]]\n27 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104473 -0.2269075 ]\n [ 0.22639626  1.1175408   0.5743505 ]]\n28 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104475 -0.2269075 ]\n [ 0.22639628  1.1175408   0.5743505 ]]\n29 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104476 -0.2269075 ]\n [ 0.22639629  1.1175408   0.5743505 ]]\n30 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104478 -0.2269075 ]\n [ 0.2263963   1.1175408   0.5743505 ]]\n31 2.5147157 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104479 -0.2269075 ]\n [ 0.22639632  1.1175408   0.5743505 ]]\n32 2.5147154 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910448  -0.2269075 ]\n [ 0.22639634  1.1175408   0.5743505 ]]\n33 2.5147154 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104482 -0.2269075 ]\n [ 0.22639635  1.1175408   0.5743505 ]]\n34 2.5147154 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104484 -0.2269075 ]\n [ 0.22639637  1.1175408   0.5743505 ]]\n35 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104485 -0.2269075 ]\n [ 0.22639638  1.1175408   0.5743505 ]]\n36 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104487 -0.2269075 ]\n [ 0.2263964   1.1175408   0.5743505 ]]\n37 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104488 -0.2269075 ]\n [ 0.22639641  1.1175408   0.5743505 ]]\n38 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910449  -0.2269075 ]\n [ 0.22639643  1.1175408   0.5743505 ]]\n39 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104491 -0.2269075 ]\n [ 0.22639644  1.1175408   0.5743505 ]]\n40 2.5147152 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104493 -0.2269075 ]\n [ 0.22639646  1.1175408   0.5743505 ]]\n41 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104494 -0.2269075 ]\n [ 0.22639647  1.1175408   0.5743505 ]]\n42 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104496 -0.2269075 ]\n [ 0.22639649  1.1175408   0.5743505 ]]\n43 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104497 -0.2269075 ]\n [ 0.2263965   1.1175408   0.5743505 ]]\n44 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104499 -0.2269075 ]\n [ 0.22639652  1.1175408   0.5743505 ]]\n45 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.191045   -0.2269075 ]\n [ 0.22639653  1.1175408   0.5743505 ]]\n46 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104502 -0.2269075 ]\n [ 0.22639655  1.1175408   0.5743505 ]]\n47 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104503 -0.2269075 ]\n [ 0.22639656  1.1175408   0.5743505 ]]\n48 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104505 -0.2269075 ]\n [ 0.22639658  1.1175408   0.5743505 ]]\n49 2.5147147 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104506 -0.2269075 ]\n [ 0.22639659  1.1175408   0.5743505 ]]\n50 2.5147145 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104508 -0.2269075 ]\n [ 0.2263966   1.1175408   0.5743505 ]]\n51 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104509 -0.2269075 ]\n [ 0.22639662  1.1175408   0.5743505 ]]\n52 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910451  -0.2269075 ]\n [ 0.22639664  1.1175408   0.5743505 ]]\n53 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104512 -0.2269075 ]\n [ 0.22639665  1.1175408   0.5743505 ]]\n54 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104514 -0.2269075 ]\n [ 0.22639666  1.1175408   0.5743505 ]]\n55 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104515 -0.2269075 ]\n [ 0.22639668  1.1175408   0.5743505 ]]\n56 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104517 -0.2269075 ]\n [ 0.2263967   1.1175408   0.5743505 ]]\n57 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104518 -0.2269075 ]\n [ 0.22639671  1.1175408   0.5743505 ]]\n58 2.5147142 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910452  -0.2269075 ]\n [ 0.22639672  1.1175408   0.5743505 ]]\n59 2.514714 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104521 -0.2269075 ]\n [ 0.22639674  1.1175408   0.5743505 ]]\n60 2.514714 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104522 -0.2269075 ]\n [ 0.22639675  1.1175408   0.5743505 ]]\n61 2.514714 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104524 -0.2269075 ]\n [ 0.22639677  1.1175408   0.5743505 ]]\n62 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104525 -0.2269075 ]\n [ 0.22639678  1.1175408   0.5743505 ]]\n63 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104527 -0.2269075 ]\n [ 0.2263968   1.1175408   0.5743505 ]]\n64 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104528 -0.2269075 ]\n [ 0.22639681  1.1175408   0.5743505 ]]\n65 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910453  -0.2269075 ]\n [ 0.22639683  1.1175408   0.5743505 ]]\n66 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104531 -0.2269075 ]\n [ 0.22639684  1.1175408   0.5743505 ]]\n67 2.5147138 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104533 -0.2269075 ]\n [ 0.22639686  1.1175408   0.5743505 ]]\n68 2.5147135 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104534 -0.2269075 ]\n [ 0.22639687  1.1175408   0.5743505 ]]\n69 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104536 -0.2269075 ]\n [ 0.22639689  1.1175408   0.5743505 ]]\n70 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104537 -0.2269075 ]\n [ 0.2263969   1.1175408   0.5743505 ]]\n71 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104539 -0.2269075 ]\n [ 0.22639692  1.1175408   0.5743505 ]]\n72 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910454  -0.2269075 ]\n [ 0.22639693  1.1175408   0.5743505 ]]\n73 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104542 -0.2269075 ]\n [ 0.22639695  1.1175408   0.5743505 ]]\n74 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104543 -0.2269075 ]\n [ 0.22639696  1.1175408   0.5743505 ]]\n75 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104545 -0.2269075 ]\n [ 0.22639698  1.1175408   0.5743505 ]]\n76 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104546 -0.2269075 ]\n [ 0.226397    1.1175408   0.5743505 ]]\n77 2.5147133 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104548 -0.2269075 ]\n [ 0.22639701  1.1175408   0.5743505 ]]\n78 2.514713 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910455  -0.2269075 ]\n [ 0.22639702  1.1175408   0.5743505 ]]\n79 2.514713 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104551 -0.2269075 ]\n [ 0.22639704  1.1175408   0.5743505 ]]\n80 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104552 -0.2269075 ]\n [ 0.22639705  1.1175408   0.5743505 ]]\n81 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104554 -0.2269075 ]\n [ 0.22639707  1.1175408   0.5743505 ]]\n82 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104555 -0.2269075 ]\n [ 0.22639708  1.1175408   0.5743505 ]]\n83 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104557 -0.2269075 ]\n [ 0.2263971   1.1175408   0.5743505 ]]\n84 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104558 -0.2269075 ]\n [ 0.22639711  1.1175408   0.5743505 ]]\n85 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910456  -0.2269075 ]\n [ 0.22639713  1.1175408   0.5743505 ]]\n86 2.5147128 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104561 -0.2269075 ]\n [ 0.22639714  1.1175408   0.5743505 ]]\n87 2.5147126 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104563 -0.2269075 ]\n [ 0.22639716  1.1175408   0.5743505 ]]\n88 2.5147126 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104564 -0.2269075 ]\n [ 0.22639717  1.1175408   0.5743505 ]]\n89 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104566 -0.2269075 ]\n [ 0.22639719  1.1175408   0.5743505 ]]\n90 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104567 -0.2269075 ]\n [ 0.2263972   1.1175408   0.5743505 ]]\n91 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104569 -0.2269075 ]\n [ 0.22639722  1.1175408   0.5743505 ]]\n92 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910457  -0.2269075 ]\n [ 0.22639723  1.1175408   0.5743505 ]]\n93 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104572 -0.2269075 ]\n [ 0.22639725  1.1175408   0.5743505 ]]\n94 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104573 -0.2269075 ]\n [ 0.22639726  1.1175408   0.5743505 ]]\n95 2.5147123 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104575 -0.2269075 ]\n [ 0.22639728  1.1175408   0.5743505 ]]\n96 2.514712 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104576 -0.2269075 ]\n [ 0.22639729  1.1175408   0.5743505 ]]\n97 2.514712 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104578 -0.2269075 ]\n [ 0.2263973   1.1175408   0.5743505 ]]\n98 2.514712 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104579 -0.2269075 ]\n [ 0.22639732  1.1175408   0.5743505 ]]\n99 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910458  -0.2269075 ]\n [ 0.22639734  1.1175408   0.5743505 ]]\n100 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104582 -0.2269075 ]\n [ 0.22639735  1.1175408   0.5743505 ]]\n101 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104584 -0.2269075 ]\n [ 0.22639737  1.1175408   0.5743505 ]]\n102 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104585 -0.2269075 ]\n [ 0.22639738  1.1175408   0.5743505 ]]\n103 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104587 -0.2269075 ]\n [ 0.2263974   1.1175408   0.5743505 ]]\n104 2.5147119 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104588 -0.2269075 ]\n [ 0.22639741  1.1175408   0.5743505 ]]\n105 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910459  -0.2269075 ]\n [ 0.22639742  1.1175408   0.5743505 ]]\n106 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104591 -0.2269075 ]\n [ 0.22639744  1.1175408   0.5743505 ]]\n107 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104593 -0.2269075 ]\n [ 0.22639745  1.1175408   0.5743505 ]]\n108 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104594 -0.2269075 ]\n [ 0.22639747  1.1175408   0.5743505 ]]\n109 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104595 -0.2269075 ]\n [ 0.22639748  1.1175408   0.5743505 ]]\n110 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104597 -0.2269075 ]\n [ 0.2263975   1.1175408   0.5743505 ]]\n111 2.5147114 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104598 -0.2269075 ]\n [ 0.22639751  1.1175408   0.5743505 ]]\n112 2.5147111 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.191046   -0.2269075 ]\n [ 0.22639753  1.1175408   0.5743505 ]]\n113 2.5147111 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104601 -0.2269075 ]\n [ 0.22639754  1.1175408   0.5743505 ]]\n114 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104603 -0.2269075 ]\n [ 0.22639756  1.1175408   0.5743505 ]]\n115 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104604 -0.2269075 ]\n [ 0.22639757  1.1175408   0.5743505 ]]\n116 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104606 -0.2269075 ]\n [ 0.22639759  1.1175408   0.5743505 ]]\n117 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104607 -0.2269075 ]\n [ 0.2263976   1.1175408   0.5743505 ]]\n118 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104609 -0.2269075 ]\n [ 0.22639762  1.1175408   0.5743505 ]]\n119 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910461  -0.2269075 ]\n [ 0.22639763  1.1175408   0.5743505 ]]\n120 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104612 -0.2269075 ]\n [ 0.22639765  1.1175408   0.5743505 ]]\n121 2.514711 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104613 -0.2269075 ]\n [ 0.22639766  1.1175408   0.5743505 ]]\n122 2.5147107 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104615 -0.2269075 ]\n [ 0.22639768  1.1175408   0.5743505 ]]\n123 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104616 -0.2269075 ]\n [ 0.2263977   1.1175408   0.5743505 ]]\n124 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104618 -0.2269075 ]\n [ 0.22639771  1.1175408   0.5743505 ]]\n125 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910462  -0.2269075 ]\n [ 0.22639772  1.1175408   0.5743505 ]]\n126 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104621 -0.2269075 ]\n [ 0.22639774  1.1175408   0.5743505 ]]\n127 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104622 -0.2269075 ]\n [ 0.22639775  1.1175408   0.5743505 ]]\n128 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104624 -0.2269075 ]\n [ 0.22639777  1.1175408   0.5743505 ]]\n129 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104625 -0.2269075 ]\n [ 0.22639778  1.1175408   0.5743505 ]]\n130 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104627 -0.2269075 ]\n [ 0.2263978   1.1175408   0.5743505 ]]\n131 2.5147104 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104628 -0.2269075 ]\n [ 0.22639781  1.1175408   0.5743505 ]]\n132 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910463  -0.2269075 ]\n [ 0.22639783  1.1175408   0.5743505 ]]\n133 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104631 -0.2269075 ]\n [ 0.22639784  1.1175408   0.5743505 ]]\n134 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104633 -0.2269075 ]\n [ 0.22639786  1.1175408   0.5743505 ]]\n135 2.5147102 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104634 -0.2269075 ]\n [ 0.22639787  1.1175408   0.5743505 ]]\n136 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104636 -0.2269075 ]\n [ 0.22639789  1.1175408   0.5743505 ]]\n137 2.5147097 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104637 -0.2269075 ]\n [ 0.2263979   1.1175408   0.5743505 ]]\n138 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104639 -0.2269075 ]\n [ 0.22639792  1.1175408   0.5743505 ]]\n139 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910464  -0.2269075 ]\n [ 0.22639793  1.1175408   0.5743505 ]]\n140 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104642 -0.2269075 ]\n [ 0.22639795  1.1175408   0.5743505 ]]\n141 2.51471 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104643 -0.2269075 ]\n [ 0.22639796  1.1175408   0.5743505 ]]\n142 2.5147097 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104645 -0.2269075 ]\n [ 0.22639798  1.1175408   0.5743505 ]]\n143 2.5147097 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104646 -0.2269075 ]\n [ 0.22639799  1.1175408   0.5743505 ]]\n144 2.5147095 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104648 -0.2269075 ]\n [ 0.226398    1.1175408   0.5743505 ]]\n145 2.5147095 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104649 -0.2269075 ]\n [ 0.22639802  1.1175408   0.5743505 ]]\n146 2.5147095 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910465  -0.2269075 ]\n [ 0.22639804  1.1175408   0.5743505 ]]\n147 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104652 -0.2269075 ]\n [ 0.22639805  1.1175408   0.5743505 ]]\n148 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104654 -0.2269075 ]\n [ 0.22639807  1.1175408   0.5743505 ]]\n149 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104655 -0.2269075 ]\n [ 0.22639808  1.1175408   0.5743505 ]]\n150 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104657 -0.2269075 ]\n [ 0.2263981   1.1175408   0.5743505 ]]\n151 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104658 -0.2269075 ]\n [ 0.22639811  1.1175408   0.5743505 ]]\n152 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910466  -0.2269075 ]\n [ 0.22639813  1.1175408   0.5743505 ]]\n153 2.5147092 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104661 -0.2269075 ]\n [ 0.22639814  1.1175408   0.5743505 ]]\n154 2.514709 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104663 -0.2269075 ]\n [ 0.22639816  1.1175408   0.5743505 ]]\n155 2.514709 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104664 -0.2269075 ]\n [ 0.22639817  1.1175408   0.5743505 ]]\n156 2.514709 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104666 -0.2269075 ]\n [ 0.22639818  1.1175408   0.5743505 ]]\n157 2.514709 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104667 -0.2269075 ]\n [ 0.2263982   1.1175408   0.5743505 ]]\n158 2.5147088 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104668 -0.2269075 ]\n [ 0.22639821  1.1175408   0.5743505 ]]\n159 2.5147088 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910467  -0.2269075 ]\n [ 0.22639823  1.1175408   0.5743505 ]]\n160 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104671 -0.2269075 ]\n [ 0.22639824  1.1175408   0.5743505 ]]\n161 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104673 -0.2269075 ]\n [ 0.22639826  1.1175408   0.5743505 ]]\n162 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104674 -0.2269075 ]\n [ 0.22639827  1.1175408   0.5743505 ]]\n163 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104676 -0.2269075 ]\n [ 0.22639829  1.1175408   0.5743505 ]]\n164 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104677 -0.2269075 ]\n [ 0.2263983   1.1175408   0.5743505 ]]\n165 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104679 -0.2269075 ]\n [ 0.22639832  1.1175408   0.5743505 ]]\n166 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910468  -0.2269075 ]\n [ 0.22639833  1.1175408   0.5743505 ]]\n167 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104682 -0.2269075 ]\n [ 0.22639835  1.1175408   0.5743505 ]]\n168 2.5147085 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104683 -0.2269075 ]\n [ 0.22639836  1.1175408   0.5743505 ]]\n169 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104685 -0.2269075 ]\n [ 0.22639838  1.1175408   0.5743505 ]]\n170 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104686 -0.2269075 ]\n [ 0.2263984   1.1175408   0.5743505 ]]\n171 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104688 -0.2269075 ]\n [ 0.22639841  1.1175408   0.5743505 ]]\n172 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910469  -0.2269075 ]\n [ 0.22639842  1.1175408   0.5743505 ]]\n173 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104691 -0.2269075 ]\n [ 0.22639844  1.1175408   0.5743505 ]]\n174 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104692 -0.2269075 ]\n [ 0.22639845  1.1175408   0.5743505 ]]\n175 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104694 -0.2269075 ]\n [ 0.22639847  1.1175408   0.5743505 ]]\n176 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104695 -0.2269075 ]\n [ 0.22639848  1.1175408   0.5743505 ]]\n177 2.514708 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104697 -0.2269075 ]\n [ 0.2263985   1.1175408   0.5743505 ]]\n178 2.5147078 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104698 -0.2269075 ]\n [ 0.22639851  1.1175408   0.5743505 ]]\n179 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.191047   -0.2269075 ]\n [ 0.22639853  1.1175408   0.5743505 ]]\n180 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104701 -0.2269075 ]\n [ 0.22639854  1.1175408   0.5743505 ]]\n181 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104703 -0.2269075 ]\n [ 0.22639856  1.1175408   0.5743505 ]]\n182 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104704 -0.2269075 ]\n [ 0.22639857  1.1175408   0.5743505 ]]\n183 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104706 -0.2269075 ]\n [ 0.22639859  1.1175408   0.5743505 ]]\n184 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104707 -0.2269075 ]\n [ 0.2263986   1.1175408   0.5743505 ]]\n185 2.5147076 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104709 -0.2269075 ]\n [ 0.22639862  1.1175408   0.5743505 ]]\n186 2.5147073 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910471  -0.2269075 ]\n [ 0.22639863  1.1175408   0.5743505 ]]\n187 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104712 -0.2269075 ]\n [ 0.22639865  1.1175408   0.5743505 ]]\n188 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104713 -0.2269075 ]\n [ 0.22639866  1.1175408   0.5743505 ]]\n189 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104715 -0.2269075 ]\n [ 0.22639868  1.1175408   0.5743505 ]]\n190 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104716 -0.2269075 ]\n [ 0.22639869  1.1175408   0.5743505 ]]\n191 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104718 -0.2269075 ]\n [ 0.2263987   1.1175408   0.5743505 ]]\n192 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104719 -0.2269075 ]\n [ 0.22639872  1.1175408   0.5743505 ]]\n193 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910472  -0.2269075 ]\n [ 0.22639874  1.1175408   0.5743505 ]]\n194 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104722 -0.2269075 ]\n [ 0.22639875  1.1175408   0.5743505 ]]\n195 2.514707 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104724 -0.2269075 ]\n [ 0.22639877  1.1175408   0.5743505 ]]\n196 2.5147066 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104725 -0.2269075 ]\n [ 0.22639878  1.1175408   0.5743505 ]]\n197 2.5147066 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104727 -0.2269075 ]\n [ 0.2263988   1.1175408   0.5743505 ]]\n198 2.5147066 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104728 -0.2269075 ]\n [ 0.22639881  1.1175408   0.5743505 ]]\n199 2.5147066 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.1910473  -0.2269075 ]\n [ 0.22639883  1.1175408   0.5743505 ]]\n200 2.5147066 [[-0.5212399  -0.58198166 -0.2430529 ]\n [-0.56880695 -0.19104731 -0.2269075 ]\n [ 0.22639884  1.1175408   0.5743505 ]]\nPrediction: [2 2 1]\nAccuracy:  0.6666667\n"
     ]
    }
   ],
   "source": [
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(201):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        print(step, cost_val, W_val)\n",
    "\n",
    "    # predict\n",
    "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
    "    # Calculate the accuracy\n",
    "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))"
   ]
  },
  {
   "source": [
    "---\n",
    "NaN이 발생하는 또 다른 경우:  \n",
    "Non-normalized inputs  \n",
    "이론적으로는 최저점을 향해 이동했으나 실제로는 그래프 밖으로 벗어난 경우.  \n",
    "\n",
    "non-normalized: 매우 큰 값이 입력으로 주어짐"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 Cost:  58554720000.0 \nPrediction:\n [[170843.67]\n [345245.9 ]\n [271323.8 ]\n [189818.94]\n [223936.5 ]\n [225832.81]\n [206888.67]\n [263754.75]]\n1 Cost:  6.4332716e+25 \nPrediction:\n [[-5.6577952e+12]\n [-1.1389723e+13]\n [-8.9598837e+12]\n [-6.2808308e+12]\n [-7.4022949e+12]\n [-7.4645981e+12]\n [-6.8415626e+12]\n [-8.7106692e+12]]\n2 Cost:  inf \nPrediction:\n [[1.8753527e+20]\n [3.7752775e+20]\n [2.9698745e+20]\n [2.0818663e+20]\n [2.4535906e+20]\n [2.4742421e+20]\n [2.2677285e+20]\n [2.8872692e+20]]\n3 Cost:  inf \nPrediction:\n [[-6.2161094e+27]\n [-1.2513667e+28]\n [-9.8440498e+27]\n [-6.9006265e+27]\n [-8.1327574e+27]\n [-8.2012093e+27]\n [-7.5166923e+27]\n [-9.5702434e+27]]\n4 Cost:  inf \nPrediction:\n [[2.0604133e+35]\n [4.1478233e+35]\n [3.2629429e+35]\n [2.2873058e+35]\n [2.6957121e+35]\n [2.7184012e+35]\n [2.4915088e+35]\n [3.1721861e+35]]\n5 Cost:  inf \nPrediction:\n [[-inf]\n [-inf]\n [-inf]\n [-inf]\n [-inf]\n [-inf]\n [-inf]\n [-inf]]\n6 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n7 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n8 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n9 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n10 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n11 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n12 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n13 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n14 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n15 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n16 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n17 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n18 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n19 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n20 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n21 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n22 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n23 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n24 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n25 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n26 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n27 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n28 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n29 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n30 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n31 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n32 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n33 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n34 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n35 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n36 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n37 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n38 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n39 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n40 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n41 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n42 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n43 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n44 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n45 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n46 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n47 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n48 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n49 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n50 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n51 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n52 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n53 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n54 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n55 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n56 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n57 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n58 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n59 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n60 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n61 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n62 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n63 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n64 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n65 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n66 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n67 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n68 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n69 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n70 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n71 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n72 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n73 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n74 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n75 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n76 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n77 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n78 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n79 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n80 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n81 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n82 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n83 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n84 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n85 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n86 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n87 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n88 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n89 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n90 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n91 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n92 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n93 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n94 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n95 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n96 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n97 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n98 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n99 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n100 Cost:  nan \nPrediction:\n [[nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]\n [nan]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5) #상당히 작은 Learning rate\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "# Launch the graph in a session.\n",
    "sess = tf.Session()\n",
    "# Initializes global variables in the graph.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(101):\n",
    "    cost_val, hy_val, _ = sess.run(\n",
    "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
    "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n"
   ]
  },
  {
   "source": [
    "cost 값은 발산하고 있고, 결과값도 너무 커지는 결과를 볼 수 있다.\n",
    "\n",
    "---\n",
    "해결책: Normalized inputs(min-max scale)  \n",
    "가장 큰 값을 1, 가장 작은 값을 0으로 하여 정규화시킴  \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.99999999 0.99999999 0.         1.         1.        ]\n [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n [0.         0.07747099 0.5326087  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "\n",
    "def min_max_scaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "\n",
    "xy = np.array(\n",
    "    [\n",
    "        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "        [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "        [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "        [819, 823, 1198100, 816, 820.450012],\n",
    "        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# very important. It does not work without it.\n",
    "xy = min_max_scaler(xy)\n",
    "print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 Cost:  2.5667095 \nPrediction:\n [[1.713187 ]\n [2.9957201]\n [2.3465576]\n [1.5928736]\n [1.9746393]\n [2.0049214]\n [1.6920049]\n [2.0588913]]\n1 Cost:  2.5665417 \nPrediction:\n [[1.7131163]\n [2.995647 ]\n [2.3464963]\n [1.5928262]\n [1.9745836]\n [2.0048676]\n [1.6919664]\n [2.0588522]]\n2 Cost:  2.566373 \nPrediction:\n [[1.7130456]\n [2.9955738]\n [2.3464353]\n [1.5927789]\n [1.9745281]\n [2.0048134]\n [1.6919279]\n [2.0588126]]\n3 Cost:  2.5662053 \nPrediction:\n [[1.7129749]\n [2.9955006]\n [2.3463743]\n [1.5927316]\n [1.9744725]\n [2.0047596]\n [1.6918895]\n [2.0587735]]\n4 Cost:  2.566037 \nPrediction:\n [[1.7129042]\n [2.9954271]\n [2.3463132]\n [1.5926843]\n [1.974417 ]\n [2.0047054]\n [1.691851 ]\n [2.0587342]]\n5 Cost:  2.565869 \nPrediction:\n [[1.7128334]\n [2.9953542]\n [2.3462522]\n [1.592637 ]\n [1.9743614]\n [2.0046515]\n [1.6918125]\n [2.0586948]]\n6 Cost:  2.565701 \nPrediction:\n [[1.7127628]\n [2.9952807]\n [2.3461912]\n [1.5925896]\n [1.9743059]\n [2.0045977]\n [1.6917741]\n [2.0586555]]\n7 Cost:  2.5655327 \nPrediction:\n [[1.7126921]\n [2.9952078]\n [2.34613  ]\n [1.5925423]\n [1.9742503]\n [2.0045435]\n [1.6917356]\n [2.0586162]]\n8 Cost:  2.5653646 \nPrediction:\n [[1.7126215]\n [2.9951344]\n [2.346069 ]\n [1.5924948]\n [1.9741945]\n [2.0044897]\n [1.6916972]\n [2.0585768]]\n9 Cost:  2.5651963 \nPrediction:\n [[1.7125506]\n [2.9950614]\n [2.3460078]\n [1.5924475]\n [1.974139 ]\n [2.0044355]\n [1.6916587]\n [2.0585375]]\n10 Cost:  2.5650287 \nPrediction:\n [[1.7124801]\n [2.994988 ]\n [2.345947 ]\n [1.5924002]\n [1.9740835]\n [2.0043817]\n [1.6916203]\n [2.0584984]]\n11 Cost:  2.5648603 \nPrediction:\n [[1.7124094]\n [2.9949148]\n [2.3458858]\n [1.592353 ]\n [1.9740279]\n [2.0043278]\n [1.6915818]\n [2.0584588]]\n12 Cost:  2.5646925 \nPrediction:\n [[1.7123387]\n [2.9948416]\n [2.345825 ]\n [1.5923055]\n [1.9739723]\n [2.0042737]\n [1.6915433]\n [2.0584197]]\n13 Cost:  2.5645242 \nPrediction:\n [[1.7122679]\n [2.9947684]\n [2.345764 ]\n [1.5922582]\n [1.9739168]\n [2.0042198]\n [1.691505 ]\n [2.0583801]]\n14 Cost:  2.5643563 \nPrediction:\n [[1.7121973]\n [2.9946952]\n [2.345703 ]\n [1.5922109]\n [1.9738612]\n [2.0041656]\n [1.6914665]\n [2.058341 ]]\n15 Cost:  2.564188 \nPrediction:\n [[1.7121266]\n [2.994622 ]\n [2.3456416]\n [1.5921636]\n [1.9738057]\n [2.0041118]\n [1.691428 ]\n [2.0583017]]\n16 Cost:  2.5640202 \nPrediction:\n [[1.7120559]\n [2.9945488]\n [2.3455808]\n [1.5921162]\n [1.97375  ]\n [2.004058 ]\n [1.6913896]\n [2.0582623]]\n17 Cost:  2.5638518 \nPrediction:\n [[1.7119851]\n [2.9944756]\n [2.3455198]\n [1.5920689]\n [1.9736944]\n [2.0040038]\n [1.691351 ]\n [2.058223 ]]\n18 Cost:  2.563684 \nPrediction:\n [[1.7119145]\n [2.9944024]\n [2.3454587]\n [1.5920215]\n [1.9736389]\n [2.0039496]\n [1.6913126]\n [2.0581837]]\n19 Cost:  2.5635161 \nPrediction:\n [[1.7118438]\n [2.994329 ]\n [2.3453975]\n [1.5919743]\n [1.9735833]\n [2.0038958]\n [1.6912742]\n [2.0581443]]\n20 Cost:  2.563348 \nPrediction:\n [[1.7117732]\n [2.994256 ]\n [2.3453367]\n [1.5919268]\n [1.9735277]\n [2.0038419]\n [1.6912357]\n [2.058105 ]]\n21 Cost:  2.5631802 \nPrediction:\n [[1.7117023]\n [2.9941826]\n [2.3452756]\n [1.5918796]\n [1.9734721]\n [2.003788 ]\n [1.6911972]\n [2.0580657]]\n22 Cost:  2.5630121 \nPrediction:\n [[1.7116318]\n [2.9941096]\n [2.3452144]\n [1.5918322]\n [1.9734166]\n [2.0037339]\n [1.6911588]\n [2.0580263]]\n23 Cost:  2.5628438 \nPrediction:\n [[1.7115611]\n [2.9940362]\n [2.3451533]\n [1.5917848]\n [1.973361 ]\n [2.0036798]\n [1.6911203]\n [2.0579872]]\n24 Cost:  2.5626764 \nPrediction:\n [[1.7114904]\n [2.9939632]\n [2.3450925]\n [1.5917375]\n [1.9733055]\n [2.0036259]\n [1.6910818]\n [2.0579479]]\n25 Cost:  2.5625083 \nPrediction:\n [[1.7114196]\n [2.9938898]\n [2.3450315]\n [1.5916902]\n [1.9732499]\n [2.003572 ]\n [1.6910434]\n [2.0579085]]\n26 Cost:  2.5623398 \nPrediction:\n [[1.711349 ]\n [2.9938166]\n [2.3449702]\n [1.5916429]\n [1.9731944]\n [2.0035179]\n [1.6910049]\n [2.0578692]]\n27 Cost:  2.5621724 \nPrediction:\n [[1.7112784]\n [2.9937437]\n [2.3449094]\n [1.5915956]\n [1.9731388]\n [2.003464 ]\n [1.6909666]\n [2.0578299]]\n28 Cost:  2.562005 \nPrediction:\n [[1.7112079]\n [2.9936705]\n [2.3448486]\n [1.5915484]\n [1.9730834]\n [2.0034103]\n [1.6909282]\n [2.0577908]]\n29 Cost:  2.5618377 \nPrediction:\n [[1.7111372]\n [2.9935975]\n [2.3447876]\n [1.5915012]\n [1.973028 ]\n [2.0033565]\n [1.6908898]\n [2.0577517]]\n30 Cost:  2.5616698 \nPrediction:\n [[1.7110667]\n [2.993524 ]\n [2.3447266]\n [1.591454 ]\n [1.9729725]\n [2.0033026]\n [1.6908516]\n [2.0577123]]\n31 Cost:  2.5615025 \nPrediction:\n [[1.7109962]\n [2.993451 ]\n [2.3446658]\n [1.5914067]\n [1.9729171]\n [2.0032487]\n [1.6908132]\n [2.0576732]]\n32 Cost:  2.5613348 \nPrediction:\n [[1.7109256]\n [2.9933782]\n [2.344605 ]\n [1.5913595]\n [1.9728616]\n [2.0031948]\n [1.6907749]\n [2.0576339]]\n33 Cost:  2.5611672 \nPrediction:\n [[1.710855 ]\n [2.993305 ]\n [2.344544 ]\n [1.5913123]\n [1.9728062]\n [2.003141 ]\n [1.6907365]\n [2.0575948]]\n34 Cost:  2.5609994 \nPrediction:\n [[1.7107844]\n [2.9932318]\n [2.344483 ]\n [1.5912652]\n [1.9727507]\n [2.003087 ]\n [1.6906981]\n [2.0575557]]\n35 Cost:  2.560832 \nPrediction:\n [[1.710714 ]\n [2.9931588]\n [2.344422 ]\n [1.5912179]\n [1.9726954]\n [2.0030332]\n [1.6906599]\n [2.0575163]]\n36 Cost:  2.5606647 \nPrediction:\n [[1.7106434]\n [2.9930859]\n [2.3443613]\n [1.5911707]\n [1.9726398]\n [2.0029793]\n [1.6906215]\n [2.0574772]]\n37 Cost:  2.560497 \nPrediction:\n [[1.7105728]\n [2.993013 ]\n [2.3443003]\n [1.5911236]\n [1.9725844]\n [2.0029254]\n [1.6905832]\n [2.057438 ]]\n38 Cost:  2.5603294 \nPrediction:\n [[1.7105023]\n [2.9929395]\n [2.3442395]\n [1.5910763]\n [1.9725289]\n [2.0028718]\n [1.6905448]\n [2.0573988]]\n39 Cost:  2.5601623 \nPrediction:\n [[1.7104318]\n [2.9928665]\n [2.3441787]\n [1.5910292]\n [1.9724735]\n [2.0028179]\n [1.6905065]\n [2.0573597]]\n40 Cost:  2.5599947 \nPrediction:\n [[1.7103612]\n [2.9927936]\n [2.3441176]\n [1.590982 ]\n [1.9724181]\n [2.002764 ]\n [1.6904682]\n [2.0573204]]\n41 Cost:  2.5598276 \nPrediction:\n [[1.7102907]\n [2.9927206]\n [2.3440568]\n [1.5909348]\n [1.9723626]\n [2.0027103]\n [1.6904299]\n [2.0572813]]\n42 Cost:  2.55966 \nPrediction:\n [[1.7102201]\n [2.9926476]\n [2.3439958]\n [1.5908875]\n [1.9723072]\n [2.0026562]\n [1.6903915]\n [2.057242 ]]\n43 Cost:  2.5594923 \nPrediction:\n [[1.7101496]\n [2.9925742]\n [2.343935 ]\n [1.5908403]\n [1.9722518]\n [2.0026026]\n [1.6903532]\n [2.0572028]]\n44 Cost:  2.559325 \nPrediction:\n [[1.7100791]\n [2.9925013]\n [2.3438742]\n [1.5907931]\n [1.9721965]\n [2.0025487]\n [1.6903149]\n [2.0571637]]\n45 Cost:  2.5591574 \nPrediction:\n [[1.7100085]\n [2.9924283]\n [2.3438132]\n [1.5907459]\n [1.9721409]\n [2.0024948]\n [1.6902765]\n [2.0571244]]\n46 Cost:  2.5589902 \nPrediction:\n [[1.7099379]\n [2.9923553]\n [2.3437524]\n [1.5906987]\n [1.9720855]\n [2.002441 ]\n [1.6902382]\n [2.0570853]]\n47 Cost:  2.558823 \nPrediction:\n [[1.7098675]\n [2.9922824]\n [2.3436916]\n [1.5906515]\n [1.9720302]\n [2.0023873]\n [1.6901999]\n [2.0570462]]\n48 Cost:  2.558656 \nPrediction:\n [[1.7097969]\n [2.9922094]\n [2.3436308]\n [1.5906043]\n [1.9719748]\n [2.0023334]\n [1.6901616]\n [2.057007 ]]\n49 Cost:  2.5584886 \nPrediction:\n [[1.7097263]\n [2.9921365]\n [2.34357  ]\n [1.5905571]\n [1.9719193]\n [2.0022795]\n [1.6901233]\n [2.056968 ]]\n50 Cost:  2.5583215 \nPrediction:\n [[1.7096558]\n [2.9920635]\n [2.3435092]\n [1.5905099]\n [1.971864 ]\n [2.0022259]\n [1.6900849]\n [2.0569289]]\n51 Cost:  2.558154 \nPrediction:\n [[1.7095853]\n [2.9919906]\n [2.3434482]\n [1.5904627]\n [1.9718086]\n [2.002172 ]\n [1.6900467]\n [2.0568895]]\n52 Cost:  2.5579867 \nPrediction:\n [[1.7095147]\n [2.9919176]\n [2.3433874]\n [1.5904156]\n [1.9717531]\n [2.002118 ]\n [1.6900084]\n [2.0568504]]\n53 Cost:  2.5578196 \nPrediction:\n [[1.7094442]\n [2.9918447]\n [2.3433266]\n [1.5903684]\n [1.9716978]\n [2.0020642]\n [1.68997  ]\n [2.0568113]]\n54 Cost:  2.5576525 \nPrediction:\n [[1.7093736]\n [2.9917717]\n [2.3432658]\n [1.5903212]\n [1.9716424]\n [2.0020106]\n [1.6899318]\n [2.0567722]]\n55 Cost:  2.557485 \nPrediction:\n [[1.7093031]\n [2.9916987]\n [2.343205 ]\n [1.590274 ]\n [1.971587 ]\n [2.0019567]\n [1.6898935]\n [2.0567331]]\n56 Cost:  2.5573182 \nPrediction:\n [[1.7092326]\n [2.9916258]\n [2.343144 ]\n [1.5902268]\n [1.9715315]\n [2.001903 ]\n [1.6898551]\n [2.056694 ]]\n57 Cost:  2.5571508 \nPrediction:\n [[1.709162 ]\n [2.9915528]\n [2.3430831]\n [1.5901796]\n [1.9714761]\n [2.0018492]\n [1.6898168]\n [2.056655 ]]\n58 Cost:  2.5569835 \nPrediction:\n [[1.7090914]\n [2.9914799]\n [2.3430223]\n [1.5901324]\n [1.9714208]\n [2.0017953]\n [1.6897784]\n [2.0566158]]\n59 Cost:  2.5568166 \nPrediction:\n [[1.709021 ]\n [2.991407 ]\n [2.3429618]\n [1.5900853]\n [1.9713653]\n [2.0017414]\n [1.6897402]\n [2.0565767]]\n60 Cost:  2.5566492 \nPrediction:\n [[1.7089504]\n [2.991334 ]\n [2.3429008]\n [1.5900381]\n [1.9713099]\n [2.0016878]\n [1.6897018]\n [2.0565376]]\n61 Cost:  2.5564818 \nPrediction:\n [[1.70888  ]\n [2.9912612]\n [2.34284  ]\n [1.5899909]\n [1.9712546]\n [2.001634 ]\n [1.6896635]\n [2.0564983]]\n62 Cost:  2.556315 \nPrediction:\n [[1.7088094]\n [2.991188 ]\n [2.3427792]\n [1.5899436]\n [1.9711993]\n [2.0015802]\n [1.6896253]\n [2.0564592]]\n63 Cost:  2.5561476 \nPrediction:\n [[1.7087389]\n [2.9911153]\n [2.3427181]\n [1.5898964]\n [1.9711438]\n [2.0015264]\n [1.6895869]\n [2.05642  ]]\n64 Cost:  2.5559807 \nPrediction:\n [[1.7086684]\n [2.9910424]\n [2.3426576]\n [1.5898494]\n [1.9710884]\n [2.0014725]\n [1.6895487]\n [2.056381 ]]\n65 Cost:  2.5558136 \nPrediction:\n [[1.7085979]\n [2.9909694]\n [2.3425968]\n [1.5898021]\n [1.971033 ]\n [2.0014186]\n [1.6895103]\n [2.056342 ]]\n66 Cost:  2.5556467 \nPrediction:\n [[1.7085273]\n [2.9908967]\n [2.342536 ]\n [1.5897549]\n [1.9709777]\n [2.001365 ]\n [1.689472 ]\n [2.0563025]]\n67 Cost:  2.5554793 \nPrediction:\n [[1.7084569]\n [2.9908235]\n [2.342475 ]\n [1.5897077]\n [1.9709222]\n [2.001311 ]\n [1.6894337]\n [2.0562634]]\n68 Cost:  2.5553126 \nPrediction:\n [[1.7083863]\n [2.9907508]\n [2.3424144]\n [1.5896606]\n [1.9708669]\n [2.0012574]\n [1.6893954]\n [2.0562243]]\n69 Cost:  2.5551453 \nPrediction:\n [[1.7083158]\n [2.9906778]\n [2.3423533]\n [1.5896133]\n [1.9708115]\n [2.0012035]\n [1.689357 ]\n [2.0561852]]\n70 Cost:  2.5549781 \nPrediction:\n [[1.7082453]\n [2.9906049]\n [2.3422925]\n [1.5895662]\n [1.970756 ]\n [2.0011497]\n [1.6893188]\n [2.0561461]]\n71 Cost:  2.5548112 \nPrediction:\n [[1.7081748]\n [2.990532 ]\n [2.3422318]\n [1.589519 ]\n [1.9707007]\n [2.001096 ]\n [1.6892805]\n [2.056107 ]]\n72 Cost:  2.554644 \nPrediction:\n [[1.7081043]\n [2.990459 ]\n [2.3421712]\n [1.5894718]\n [1.9706453]\n [2.0010421]\n [1.6892421]\n [2.056068 ]]\n73 Cost:  2.554477 \nPrediction:\n [[1.7080338]\n [2.990386 ]\n [2.3421102]\n [1.5894246]\n [1.9705899]\n [2.0009882]\n [1.6892039]\n [2.0560288]]\n74 Cost:  2.5543098 \nPrediction:\n [[1.7079632]\n [2.990313 ]\n [2.3420494]\n [1.5893775]\n [1.9705346]\n [2.0009346]\n [1.6891656]\n [2.0559897]]\n75 Cost:  2.554143 \nPrediction:\n [[1.7078928]\n [2.99024  ]\n [2.3419886]\n [1.5893303]\n [1.9704791]\n [2.0008807]\n [1.6891272]\n [2.0559504]]\n76 Cost:  2.5539758 \nPrediction:\n [[1.7078222]\n [2.9901671]\n [2.3419275]\n [1.5892831]\n [1.9704238]\n [2.0008268]\n [1.689089 ]\n [2.0559115]]\n77 Cost:  2.5538087 \nPrediction:\n [[1.7077518]\n [2.9900942]\n [2.341867 ]\n [1.5892359]\n [1.9703684]\n [2.0007732]\n [1.6890506]\n [2.0558722]]\n78 Cost:  2.5536418 \nPrediction:\n [[1.7076812]\n [2.9900212]\n [2.3418062]\n [1.5891888]\n [1.9703131]\n [2.0007193]\n [1.6890123]\n [2.055833 ]]\n79 Cost:  2.5534747 \nPrediction:\n [[1.7076107]\n [2.9899483]\n [2.3417454]\n [1.5891416]\n [1.9702575]\n [2.0006657]\n [1.688974 ]\n [2.055794 ]]\n80 Cost:  2.5533078 \nPrediction:\n [[1.7075403]\n [2.9898753]\n [2.3416846]\n [1.5890944]\n [1.9702022]\n [2.0006118]\n [1.6889358]\n [2.055755 ]]\n81 Cost:  2.5531406 \nPrediction:\n [[1.7074697]\n [2.9898026]\n [2.3416235]\n [1.5890472]\n [1.9701469]\n [2.000558 ]\n [1.6888974]\n [2.0557158]]\n82 Cost:  2.5529737 \nPrediction:\n [[1.7073992]\n [2.9897296]\n [2.341563 ]\n [1.589    ]\n [1.9700916]\n [2.0005043]\n [1.6888591]\n [2.0556765]]\n83 Cost:  2.552807 \nPrediction:\n [[1.7073288]\n [2.989657 ]\n [2.3415022]\n [1.5889529]\n [1.970036 ]\n [2.0004506]\n [1.6888207]\n [2.0556374]]\n84 Cost:  2.55264 \nPrediction:\n [[1.7072582]\n [2.989584 ]\n [2.3414412]\n [1.5889057]\n [1.9699807]\n [2.0003967]\n [1.6887825]\n [2.0555983]]\n85 Cost:  2.552473 \nPrediction:\n [[1.7071878]\n [2.989511 ]\n [2.3413806]\n [1.5888585]\n [1.9699254]\n [2.0003428]\n [1.6887442]\n [2.0555592]]\n86 Cost:  2.552306 \nPrediction:\n [[1.7071173]\n [2.989438 ]\n [2.3413196]\n [1.5888114]\n [1.9698701]\n [2.000289 ]\n [1.6887059]\n [2.05552  ]]\n87 Cost:  2.5521393 \nPrediction:\n [[1.7070469]\n [2.989365 ]\n [2.341259 ]\n [1.5887642]\n [1.9698147]\n [2.0002353]\n [1.6886675]\n [2.055481 ]]\n88 Cost:  2.5519722 \nPrediction:\n [[1.7069764]\n [2.9892921]\n [2.341198 ]\n [1.5887171]\n [1.9697592]\n [2.0001814]\n [1.6886293]\n [2.0554419]]\n89 Cost:  2.5518053 \nPrediction:\n [[1.7069058]\n [2.9892192]\n [2.3411374]\n [1.5886699]\n [1.9697039]\n [2.0001278]\n [1.6885909]\n [2.0554028]]\n90 Cost:  2.5516384 \nPrediction:\n [[1.7068354]\n [2.9891462]\n [2.3410766]\n [1.5886227]\n [1.9696485]\n [2.000074 ]\n [1.6885526]\n [2.0553637]]\n91 Cost:  2.5514712 \nPrediction:\n [[1.7067649]\n [2.9890735]\n [2.3410156]\n [1.5885756]\n [1.9695932]\n [2.00002  ]\n [1.6885144]\n [2.0553243]]\n92 Cost:  2.5513043 \nPrediction:\n [[1.7066944]\n [2.9890006]\n [2.3409548]\n [1.5885284]\n [1.9695379]\n [1.9999664]\n [1.6884761]\n [2.0552852]]\n93 Cost:  2.551138 \nPrediction:\n [[1.7066239]\n [2.9889278]\n [2.3408942]\n [1.5884812]\n [1.9694824]\n [1.9999126]\n [1.6884377]\n [2.055246 ]]\n94 Cost:  2.5509708 \nPrediction:\n [[1.7065535]\n [2.988855 ]\n [2.3408332]\n [1.588434 ]\n [1.9694271]\n [1.9998589]\n [1.6883994]\n [2.055207 ]]\n95 Cost:  2.5508041 \nPrediction:\n [[1.706483 ]\n [2.988782 ]\n [2.3407726]\n [1.5883869]\n [1.9693717]\n [1.9998051]\n [1.688361 ]\n [2.055168 ]]\n96 Cost:  2.550637 \nPrediction:\n [[1.7064126]\n [2.988709 ]\n [2.3407116]\n [1.5883397]\n [1.9693162]\n [1.9997513]\n [1.6883228]\n [2.0551288]]\n97 Cost:  2.5504699 \nPrediction:\n [[1.706342 ]\n [2.988636 ]\n [2.340651 ]\n [1.5882925]\n [1.9692609]\n [1.9996974]\n [1.6882845]\n [2.0550895]]\n98 Cost:  2.5503032 \nPrediction:\n [[1.7062715]\n [2.988563 ]\n [2.34059  ]\n [1.5882454]\n [1.9692056]\n [1.9996438]\n [1.6882463]\n [2.0550504]]\n99 Cost:  2.5501363 \nPrediction:\n [[1.7062011]\n [2.98849  ]\n [2.3405292]\n [1.5881982]\n [1.9691502]\n [1.9995899]\n [1.6882079]\n [2.0550113]]\n100 Cost:  2.5499694 \nPrediction:\n [[1.7061305]\n [2.9884174]\n [2.3404684]\n [1.588151 ]\n [1.9690948]\n [1.9995362]\n [1.6881696]\n [2.0549722]]\n"
     ]
    }
   ],
   "source": [
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "# placeholders for a tensor that will be always fed.\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "# Hypothesis\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "# Simplified cost/loss function\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "# Minimize\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session.\n",
    "with tf.Session() as sess:\n",
    "    # Initializes global variables in the graph.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(101):\n",
    "        _, cost_val, hy_val = sess.run(\n",
    "            [train, cost, hypothesis], feed_dict={X: x_data, Y: y_data}\n",
    "        )\n",
    "        print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)"
   ]
  }
 ]
}